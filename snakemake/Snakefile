import re
from typing import List

configfile: "config.yaml"

# whether we run snakemake locally
is_local = config.get('local',False)

# currently we are using macOS locally
is_macos = is_local

# whether we are using macOS
is_macos = config.get('macos',is_macos)

if is_macos:
    # we installed it locally
    dev_env = 'envs/dev.yaml'
else:
    # this only works on Linux
    dev_env = 'envs/dev_with_polydfe.yaml'


def extract_opt(str: str, name, default_value=None) -> str:
    """
    Extract named option from string like 'opt1_5.opt2_test' wh

    :param str: String to extract from
    :param name: Name of option
    :param default_value: Default value if option is not found
    :return: Extracted option or default value
    """
    # named options have the following signature
    match = re.search(f"[./-]{name}[_:]([^./-]*)",str)

    if match:
        return match.groups()[0]

    return default_value


def expand_comparisons(path) -> (List[str], List[str]):
    """
    Expand over the comma separated lists in 'path'

    :param path: Path to expand
    :return: Expanded paths and labels
    """
    matches = re.finditer(r"\[(.*?)\]",path)

    def expand(strings: List[str], match: re.Match) -> (List[str], List[str]):
        """
        Expand by substituting first match in given strings.
        Also attempt to parse labels enclosed in <> tags.

        :param strings: Strings to expand
        :param match: Match to expand with
        :return: Expanded strings and labels
        """
        elements = match.groups()[0].split(',')

        expanded, labels = [], []
        for string in strings:
            for e in elements:
                # attempt to match explicit label
                if matched_name := re.search("<(.*)>",e):
                    # remove label tags
                    e = re.sub("<.*>",'',e)

                    label = matched_name.groups()[0]
                else:
                    label = None

                expanded.append(re.sub(match.re.pattern,e,string,1))
                labels.append(label)

        return expanded, labels

    expanded = [path]
    labels = []
    for match in matches:
        expanded, labels = expand(expanded,match)

    # set labels to None if first one evaluates to false
    if not labels[0]:
        labels = None

    # prepend base path to strings
    return ["results/graphs/" + s for s in expanded], labels


configs = expand(
    "{species}_{model}_{type}",
    species=['pendula', 'pubescens', 'example_1', 'example_2', 'example_3'],
    model=['C'],
    type=['full', 'full_anc', 'deleterious', 'deleterious_anc']
)

# model mapping
polydfe_models = {
    'A': 'DisplacedGammaParametrization',
    'B': 'GammaDiscreteParametrization',
    'C': 'GammaExpParametrization',
    'D': 'DiscreteParametrization'
}

ruleorder:
    merge_bootstraps_polyDFE > infer_dfe_polyDFE >
    create_config > create_config_from_spectra >

wildcard_constraints:
    opts=r'[^/]*',# match several optional options not separated by /
    chr=r'\w+',
    pop="[a-zA-Z]+",
    param="[a-zA-Z0-9_]+",
    config=r'\w+',
    population=r'\w+',
    demography=r'\w+',
    chunk=r'\d+',
    n=r'\d+',
    i=r'\d+',
    folded='folded|unfolded',


hominidae = [
    'bonobo',
    'bornean_orang',
    'central_chimp',
    'eastern_chimp',
    'human',
    'NC_chimp',
    'sumatran_orang',
    'western_chimp',
    'western_lowland_gorilla'
]

# configuration for DFE comparison collages
configs_collages = dict(
    strong_del=dict(n_chunks=[40],L=["1e7"],mu=["1e-8"],s_b=["1e-9"],b=[0.1, 1, 10],s_d=["1e-2", "1e-1", "1e0", "1e1"],p_b=[0],demography=["constant"]),
    strong_bene=dict(n_chunks=[40],L=["1e7"],mu=["1e-8"],s_b=["1e-4", "1e-2", "1e0"],b=[0.4],s_d=["1e0"],p_b=[0, 0.2, 0.4],demography=["constant"]),
    realistic_full_dfe=dict(n_chunks=[40],L=["1e7"],mu=["1e-8"],s_b=["1e-4", "1e-3"],b=[0.4],s_d=["1e-1", "1e0"],p_b=[0.01, 0.05],demography=["constant"]),
    realistic_del_dfe=dict(n_chunks=[100],L=["1e7"],mu=["1e-8"],s_b=["1e-9"],b=[3, 1, 0.3],s_d=["3e-3", "3e-2", "3e-1"],p_b=[0],demography=["constant"]),
)

collages = {
    k: expand(
        "results/graphs/comp/slim/n_replicate={{n_replicate}}/n_chunks={n_chunks}/g={{g}}/L={L}/mu={mu}/r={{r}}/N={{N}}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={{n}}/{{demography}}/{{folded}}/{{plot_type}}.png",
        **v
    )
    for k, v in configs_collages.items()
}


collages["mixed"] = [
    "results/graphs/comp/slim/n_replicate={{n_replicate}}/n_chunks=100/g={{g}}/L=1e7/mu=1e-8/r={{r}}/N={{N}}/s_b=1e-3/b=0.3/s_d=3e-1/p_b=0.00/n={{n}}/{{demography}}/{{folded}}/{{plot_type}}.png",
    "results/graphs/comp/slim/n_replicate={{n_replicate}}/n_chunks=100/g={{g}}/L=1e7/mu=1e-8/r={{r}}/N={{N}}/s_b=1e-3/b=0.1/s_d=3e-2/p_b=0.00/n={{n}}/{{demography}}/{{folded}}/{{plot_type}}.png",
    "results/graphs/comp/slim/n_replicate={{n_replicate}}/n_chunks=100/g={{g}}/L=1e7/mu=1e-8/r={{r}}/N={{N}}/s_b=1e-3/b=0.1/s_d=3e-1/p_b=0.01/n={{n}}/{{demography}}/{{folded}}/{{plot_type}}.png",
    "results/graphs/comp/slim/n_replicate={{n_replicate}}/n_chunks=100/g={{g}}/L=1e7/mu=1e-8/r={{r}}/N={{N}}/s_b=1e-4/b=0.3/s_d=3e-2/p_b=0.05/n={{n}}/{{demography}}/{{folded}}/{{plot_type}}.png",
]

rule all:
    input:
        (
            #"../docs/_build"
            #"results/slim/L=100000.trees",
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/dfe.png", n_replicate=[1], g=[10], n=[20], L=[int(1e10)], mu=[1e-8], N=[1000], s_b=[0, 0.01, 0.1], b=[1], s_d=[0, 0.01, 0.1], p_b=[0.05, 0.2]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/dfe.png", n_replicate=[1], g=[1000, 10000], n=[20], L=[int(1e9)], mu=[1e-9], r=[1e-9], N=[1000], s_b=[0], b=[1], s_d=[0], p_b=[0.05]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/dfe.png",
            #    n_replicate=[1, 2],g=[10000],n=[10, 20],L=[int(1e9)],mu=[1e-9],
            #    r=[1e-9],N=[10000],s_b=[0, 0.05],b=[1],s_d=[0, 0.1],p_b=[0.2]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/dfe.png",
            #    n_replicate=[1],g=["1e4", "2e4", "4e4", "8e4", "16e4"],n=[10],L=["1e8"],mu=["1e-9"],
            #    r=["1e-9"],N=["1e4"],s_b=["5e-3"],b=[1],s_d=["1e-4"],p_b=[0.2]),
            #expand("results/graphs/spectra/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/sfs.png",
            #    n_replicate=[1],g=["1e4", "2e4", "4e4", "8e4", "16e4"],n=[10],L=["1e8"],mu=["1e-9"],
            #    r=["1e-9"],N=["1e4"],s_b=["1e-9"],b=[1],s_d=["1e-4"],p_b=[0.2]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/sfs.png",
            #    n_replicate=[1, 2],n_chunks=[20],g=["1e4", "2e4"],n=[20],L=["1e8"],mu=["1e-8"],
            #    r=["1e-7"],N=["1e3"],s_b=["1e-9"],b=[0.1, 1, 10],s_d=["1e-2", "1e-1", "1e0", "1e1"],p_b=[0]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/collage1.{type}.png", n_replicate=[1], type=['dfe', 'sfs'], g=["2e4"], N=["2e3"]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/collage1.{type}.png", n_replicate=[1], type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-7"]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/collage2.{type}.png", n_replicate=[1], type=['dfe', 'sfs'], g=["1e4"], N=["1e3"]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/collage2.{type}.png", n_replicate=[1], type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-7"]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/{folded}/collage/{collage_type}.{plot_type}.png", n_replicate=[1], plot_type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-7"], n=[20, 100], folded=['folded', 'unfolded'], collage_type=['realistic_full_dfe', 'realistic_del_dfe']),
            expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/{demography}/{folded}/collage/{collage_type}.{plot_type}.png", n_replicate=[1, 2], plot_type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-7"], n=[20], folded=['unfolded'], collage_type=['mixed'], demography=['constant', 'expansion', 'bottleneck']),
            expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/{demography}/{folded}/collage/{collage_type}.{plot_type}.png", n_replicate=[1], plot_type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-8", "1e-6"], n=[20], folded=['unfolded'], collage_type=['mixed'], demography=['expansion']),
            expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/{demography}/{folded}/collage/{collage_type}.{plot_type}.png", n_replicate=[1], plot_type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-7"], n=[20], folded=['folded'], collage_type=['mixed'], demography=['constant', 'expansion', 'bottleneck']),
            expand("results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/{demography}/{folded}/collage/{collage_type}.{plot_type}.png", n_replicate=[1], plot_type=['dfe', 'sfs'], g=["1e4"], N=["1e3"], r=["1e-7"], n=[100], folded=['unfolded'], collage_type=['mixed'], demography=['constant', 'expansion', 'bottleneck']),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/sfs.png",
            #        n_replicate=[1],n_chunks=[100],n=[20],L=["1e7"],mu=["1e-8"],s_b=["1e0"],b=[0.4],s_d=["1e0"],p_b=[0.4],g=["1e4", "2e4", "4e4"],r=["1e-6"],N=["1e3"]),
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/sfs.png",
            #    n_replicate=[1],n_chunks=[200],n=[20],L=["5e6"],mu=["1e-8"],s_b=["1e0"],b=[0.4],s_d=["1e0"],p_b=[0.4],g=["1e4"],r=["1e-5"],N=["1e3"])
            #expand("results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/dfe.png",
            #    n_replicate=[1, 2],g=[10000],n=[20],L=[int(1e10)],mu=[1e-9],
            #    r=[1e-9],N=[1000],s_b=[0.01],b=[1, 10],s_d=[0.1],p_b=[0.1]),
            #expand("results/graphs/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.params.png", param=['S_d', 'b'], col=['mean.rsa', 'mean.DGrantham', 'mean.dis'], n_bins=[5, 10, 15], subsample_size=[10, 20, 30]),
            #expand("results/sfs/betula/{sample_set}/{n}/sfs.csv", sample_set=['pendula', 'pubescens'], n=[10, 20]),
            #expand("results/sfs/betula/{sample_set}/{n}/sfs.polarized-on-the-fly.csv", sample_set=['pendula', 'pubescens'], n=[10, 20]),
            #expand("results/vcf/betula/{name}.vcf.gz",name=['biallelic.polarized',
            #                                                'biallelic.polarized.subset.10000',
            #                                                'biallelic.polarized.subset.50000',
            #                                                'all.polarized',
            #                                                'all.polarized.subset.10000',
            #                                                'all.polarized.subset.200000']),
            #expand("results/sfs/betula/{sample_set}/{n}/sfs.csv", sample_set=['pendula', 'pubescens'], n=[20]),
            #expand("results/graphs/comp/spectra/hgdp/inference/{chr}/{opts}/[all,Japanese].png",chr=[21, 1], opts=['opts.subset.50000.n.10', 'opts.subset.50000.n.20']),
            #expand("results/graphs/comp/pca/hgdp/inference/{chr}/{opts}/[all,Japanese].png",chr=[21], opts=['opts.subset.50000']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[10,20].config.{config}/[all,Japanese].dfe.png",chr=[1, 2, 3], opts=['opts'], config=['default']),
            #expand("results/graphs/comp/[fastdfe,polydfe]/hgdp/inference/{chr}/{opts}.n.10.config.{config}/[all].dfe.png",chr=[1, 2, 21, 22],opts=['opts'],config=['default']),
            #expand("results/graphs/comp/[fastdfe,polydfe]/hgdp/inference/{chr}/{opts}.n.[5,10,15,20,25,30,35,40].config.{config}/[all].dfe.png",chr=[1, 2, 3, 'all'],opts=['opts'],config=['default']),
            #expand("results/graphs/comp/spectra/hgdp/{chr}/{opts}.n.[5,10,15,20,25,30,35,40]/[all].png",chr=[1, 2, 3, 4, 5, 'all'],opts=['opts'],config=['default']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[5,10,15,20,25,30,35,40].config.{config}/[all].dfe.png",chr=[1, 2, 3, 'all'],opts=['opts'],config=['deleterious']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[5,10,15,20,25,30,35,40].config.{config}/[all].dfe.png",chr=[1],opts=['opts'],config=['deleterious']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[10].config.{config}/[all].dfe.png",chr=[8, 9],opts=['opts'],config=['deleterious']),
            #expand("results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.10.config.{config}/all.dfe.png",chr=[21],opts=['opts.vep'],config=['default']),
            #expand("results/graphs/comp/fastdfe/hominidae/[{types}].dfe.png",types=','.join(hominidae)),
            #expand("results/graphs/comp/fastdfe/hominidae/[{types}].params.png",types=','.join(hominidae)),
            #expand("results/graphs/fastdfe/hominidae/cov/{param}.params.png", param=['S_d', 'eps', 'p_b', 'S_b', 'b']),
            #expand("results/graphs/comp/fastdfe/hominidae/cov/[S_d,eps,p_b,S_b,b].params.png"),
            #expand("results/graphs/comp/fastdfe/hominidae/cov/[S_d,eps,p_b,S_b,b].dfe.png")
            #expand("results/graphs/comp/fastdfe/hgdp/{chr}/{opts}.subset.50000.n.[10,20].config.{config}/[all,Japanese].dfe.png",chr=[21], opts=['opts'], config=['test']),
            #expand("results/configs/{config}_bootstrapped_100/config.yaml",config=configs),
            #expand("results/fastdfe/{config}_bootstrapped_100/serialized.json",config=configs),
            #expand("results/configs/{config}/config.yaml",config=['pendula.pubescens.example_1.example_2.example_3_C_full_anc']),
            #expand("results/graphs/fastdfe/{config}_bootstrapped_100/dfe_discretized.png",config=configs),
            #expand("results/spectra/{species}.csv", species=['pendula.pubescens', 'pendula.pubescens.example_1.example_2.example_3'])
            #expand("results/polydfe/{config}_bootstrapped_100/serialized.json",config=configs),
            #expand("results/graphs/comp/[fastdfe<fastDFE>,polydfe<polyDFE>]/{config}_bootstrapped_100/{type}.png",
            #    config=configs,
            #    type=['mle_params', 'dfe_discretized']
            #),
            #expand("results/graphs/comp/[fastdfe<fastDFE>,polydfe<polyDFE>]/{config}/{type}.png",
            #    config=['pendula_C_full'],
            #    type=['mle_params', 'dfe_discretized']
            #),
            #expand("results/graphs/{tool}/{config}/sfs_comparison.png",config=configs, tool=['fastdfe', 'polydfe']),
            #expand("results/graphs/comp/tool_comparison_collage/{type}.png",type=['discretized', 'inferred_params']),
            #expand("results/graphs/comp/fastdfe/{type}_collage.png",type=['sfs_comparison_detailed']),
            #expand("results/fastdfe/{config}_bootstrapped_sequential_100/serialized.json",config=configs),
            #expand("results/spectra/betula/pendula/{s}/all.csv",
            #    s=[
            #        'DegeneracyStratification',
            #        'DegeneracyStratification.BaseContextStratification',
            #        'DegeneracyStratification.BaseTransitionStratification',
            #        'DegeneracyStratification.BaseContextStratification.BaseTransitionStratification'
            #    ]
            #)
            #expand(f"results/graphs/tool_comp/pendula_C_full_bootstrapped_10/discretized.png"),
            #expand("results/fastdfe/{species}_{model}_{type}/config.yaml",species=['pendula', 'pubescens'],model=['C'],type=['full', 'full_anc']),
            #expand("results/graphs/fastdfe/{config}/dfe_discretized.png",config=['pendula_C_full', 'pendula_C_full_anc', 'pubescens_C_full', 'pubescens_C_full_anc']),
            #expand("results/polydfe/{config}/bootstrapped_{n}/serialized.json",config=['pendula_C_full_anc'],n=10),

            # tests
            #expand("results/testing/{config}_bootstrapped_100/overlapping_confidence_intervals.txt",config=configs)
            #expand("results/graphs/comp/spectra/[default,vep,vep_biallelic,vep_biallelic_coding]/hgdp/1/opts.n.10/all.png")
            # "../docs/requirements.txt"
        )

if is_macos:
    rule fetch_polyDFE_binary:
        output:
            "../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit"
        params:
            url="https://github.com/paula-tataru/polyDFE/blob/master/polyDFE-2.0-macOS-64-bit?raw=true"
        conda:
            "envs/wget.yaml"
        shell:
            """
                wget {params.url} -O {output}
                chmod +x {output}
            """

# fetch the postprocessing script from the GitHub repository
rule fetch_postprocessing_script_polyDFE:
    output:
        config['polydfe_postprocessing_source']
    params:
        url="https://github.com/paula-tataru/polyDFE/raw/master/postprocessing.R"
    conda:
        "envs/wget.yaml"
    shell:
        "wget {params.url} -O {output}"

# create a configuration file for DFE inference
rule create_config:
    input:
        sfs="../resources/polydfe/{species}/spectra/sfs.txt",
        init="../resources/polydfe/init/{model}.{type}_init"
    output:
        "results/configs/{species,pendula|pubescens|example_1|example_2|example_3}_{model,[ABCD]}_{type,full_anc|full|deleterious|deleterious_anc}/config.yaml"
    params:
        model=lambda w: polydfe_models[w.model]
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config.py"

# modify an existing config file
rule modify_config_do_bootstrap:
    input:
        "results/configs/{config}/config.yaml"
    output:
        r"results/configs/{config}_bootstrapped_{n,\d+}/config.yaml"
    params:
        opts=lambda w: dict(
            do_bootstrap=True,
            n_bootstraps=int(w.n)
        )
    conda:
        "envs/base.yaml"
    script:
        "scripts/modify_config.py"

# modify an existing config file
rule modify_config_do_bootstrap_sequential:
    input:
        "results/configs/{config}/config.yaml"
    output:
        r"results/configs/{config}_bootstrapped_sequential_{n,\d+}/config.yaml"
    params:
        opts=lambda w: dict(
            do_bootstrap=True,
            n_bootstraps=int(w.n),
            parallelize=False
        )
    conda:
        "envs/base.yaml"
    script:
        "scripts/modify_config.py"

# modify an existing config file
rule modify_config_seeded:
    input:
        "results/configs/{config}/config.yaml"
    output:
        "results/configs/{config}_seeded_{seed}/config.yaml"
    params:
        opts=lambda w: dict(
            seed=int(w.seed)
        )
    conda:
        "envs/base.yaml"
    script:
        "scripts/modify_config.py"

# infer DFE from SFS
rule infer_dfe:
    input:
        "results/configs/{config}/config.yaml"
    output:
        summary="results/fastdfe/{config}/summary.json",
        serialized="results/fastdfe/{config}/serialized.json"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe.py"

# infer DFE from SFS using polyDFE
rule infer_dfe_polyDFE:
    input:
        "results/configs/{config}/config.yaml",
        config['polydfe_postprocessing_source']
    output:
        polydfe="results/polydfe/{config}/out.txt",
        summary="results/polydfe/{config}/summary.json",
        serialized="results/polydfe/{config}/serialized.json"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# infer DFE from bootstrapped SFS using polyDFE
rule infer_dfe_polyDFE_bootstrap:
    input:
        "results/polydfe/{config}/bootstraps/{i}/config.yaml",
        config['polydfe_postprocessing_source']
    output:
        polydfe="results/polydfe/{config}/bootstraps/{i}/out.txt",
        summary="results/polydfe/{config}/bootstraps/{i}/summary.json",
        serialized="results/polydfe/{config}/bootstraps/{i}/serialized.json"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# infer DFE from SFS
rule plot_DFE_inference:
    input:
        "results/fastdfe/{config}/serialized.json"
    output:
        dfe_discretized="results/graphs/fastdfe/{config}/dfe_discretized.png",
        dfe_continuous="results/graphs/fastdfe/{config}/dfe_continuous.png",
        sfs_comparison="results/graphs/fastdfe/{config}/sfs_comparison.png",
        sfs_comparison_detailed="results/graphs/fastdfe/{config}/sfs_comparison_detailed.png",
        sfs_input="results/graphs/fastdfe/{config}/sfs_input.png",
        mle_params="results/graphs/fastdfe/{config}/mle_params.png",
        bucket_sizes="results/graphs/fastdfe/{config}/bucket_sizes.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/plot_base_inference.py"

# create config file for bootstrapped samples to be run with polyDFE
rule create_bootstrap_polyDFE:
    input:
        "results/polydfe/{config}/serialized.json",
        config['polydfe_postprocessing_source']
    output:
        "results/polydfe/{config}/bootstraps/{i}/config.yaml"
    conda:
        dev_env
    script:
        "scripts/create_bootstrap_polydfe.py"

# merge bootstraps and original inference result
rule merge_bootstraps_polyDFE:
    input:
        config['polydfe_postprocessing_source'],
        original="results/polydfe/{config}/serialized.json",
        bootstraps=lambda w: expand("results/polydfe/{config}/bootstraps/{i}/serialized.json",
            i=range(int(w.n)),allow_missing=True)
    output:
        "results/polydfe/{config}_bootstrapped_{n}/serialized.json"
    conda:
        dev_env
    script:
        "scripts/merge_bootstraps_polydfe.py"

# infer DFE from SFS
rule plot_polyDFE_inference:
    input:
        "results/polydfe/{config}/serialized.json"
    output:
        dfe_discretized="results/graphs/polydfe/{config}/dfe_discretized.png",
        mle_params="results/graphs/polydfe/{config}/mle_params.png"
    conda:
        dev_env
    script:
        "scripts/plot_polydfe_inference.py"

# combine a number of plots in a new plot
# comma-separated strings are expanded
rule combine_plots:
    input:
        lambda w: expand_comparisons(w.path)[0]
    output:
        "results/graphs/comp/{path}"
    params:
        titles=lambda w: expand_comparisons(w.path)[1]
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# test whether the confidence intervals of the discretized DFEs overlap
rule test_overlapping_confidence_intervals:
    input:
        fastdfe='results/fastdfe/{config}/serialized.json',
        polydfe='results/polydfe/{config}/serialized.json'
    output:
        touch("results/testing/{config}/overlapping_confidence_intervals.txt")
    conda:
        dev_env
    script:
        "scripts/check_overlapping_confidence_intervals.py"

# copy to cache for testing
rule copy_to_testing:
    input:
        "results/{all}"
    output:
        "../testing/{all}"
    conda:
        "envs/base.yaml"
    shell:
        "cp -r {input} {output}"

# renew test cache used for comparison
rule renew_test_file_cache_fastdfe:
    input:
        "../testing/configs/pendula_C_full_anc/config.yaml",
        "../testing/configs/pendula.pubescens.example_1.example_2.example_3_C_full_anc/config.yaml",
        "../testing/fastdfe/pendula_C_full_anc/serialized.json",
        "../testing/fastdfe/templates/shared/shared_example_1/serialized.json",
        "../testing/fastdfe/templates/shared/shared_example_2/serialized.json",
        "../testing/fastdfe/templates/shared/covariates_dummy_example_1/serialized.json",
        "../testing/fastdfe/templates/shared/covariates_Sd_example_1/serialized.json"
    output:
        touch("../testing/renew_cache_fastdfe.txt")

# renew test cache used for comparison
rule renew_test_file_cache_polydfe:
    input:
        "../testing/polydfe/pendula_C_full_anc/serialized.json"
    output:
        touch("../testing/renew_cache_polydfe.txt")

# renew test cache used for comparison
rule renew_test_file_cache:
    input:
        "../testing/renew_cache_fastdfe.txt",
        "../testing/renew_cache_polydfe.txt"
    output:
        touch("../testing/renew_cache.txt")

# combine spectra from several species
rule merge_spectra_species:
    input:
        lambda w: expand("../resources/polydfe/{s}/spectra/sfs.txt",s=w.species.split('.'))
    output:
        "results/spectra/{species}.csv"
    params:
        names=lambda w: w.species.split('.')
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_spectra.py"

# create a configuration file from a serialized spectra object
rule create_config_from_spectra:
    input:
        spectra="results/spectra/{species}.csv",
        # take init file for first species
        init=lambda w: f"../resources/polydfe/init/{w.model}.{w.type}_init"
    output:
        "results/configs/{species}_{model,[ABCD]}_{type,full_anc|full|deleterious|deleterious_anc}/config.yaml"
    params:
        model=lambda w: polydfe_models[w.model]
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config_from_spectra.py"

# plot tool comparison
rule plot_tool_comparison:
    input:
        fastdfe='results/fastdfe/{config}/serialized.json',
        polydfe='results/polydfe/{config}/serialized.json'
    output:
        discretized='results/graphs/tool_comp/{config}/discretized.png',
        inferred_params='results/graphs/tool_comp/{config}/inferred_params.png'
    conda:
        dev_env
    script:
        "scripts/plot_tool_comparison.py"

# plot fastdfe/polydfe collage
# file names get too long when trying to use combine_plots
rule plot_tool_comparison_collage:
    input:
        lambda w:
        expand_comparisons(f"tool_comp/[{','.join([f'{c}_bootstrapped_100' for c in configs])}]/{w.type}.png")[0]
    output:
        "results/graphs/comp/tool_comparison_collage/{type}.png"
    params:
        titles=lambda w: [c.replace('_C_','_').replace('_',' ') for c in configs],
        title_size_rel=50,
        n_cols=4,
        n_rows=5,
        figsize=(16, 15),
        dpi=400
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# plot fastdfe sfs comparison collage
rule plot_sfs_comparison_collage:
    input:
        lambda w:
        expand_comparisons(f"fastdfe/[{','.join([f'{c}_bootstrapped_100' for c in configs])}]/{w.type}.png")[0]
    output:
        "results/graphs/comp/fastdfe/{type}_collage.png"
    params:
        titles=lambda w: [c.replace('_C_','_').replace('_',' ') for c in configs],
        title_size_rel=50,
        n_cols=4,
        n_rows=5,
        figsize=(16, 15),
        dpi=400
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# create a tbi index for a vcf file
rule create_tbi_vcf:
    input:
        "{path}.vcf.gz"
    output:
        "{path}.vcf.gz.tbi"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk IndexFeatureFile -I {input}"

# create a tbi index for a gff file
rule create_tbi_gff:
    input:
        "{path}.{ext}.gz"
    output:
        "{path}.{ext,gff|gtf|gff3}.gz.tbi"
    conda:
        "envs/tabix.yaml"
    shell:
        "tabix -p gff {input}"

# create a tbi index for a gff file
"""
rule bgzip_gff:
    input:
        "{path}.{ext}"
    output:
        "{path}.{ext,gff|gtf|gff3}.gz"
    conda:
        "envs/tabix.yaml"
    shell:
        "bgzip -c {input} > {input}.gz"
"""

# index a fasta file using samtools
rule create_fai:
    input:
        "{path}.fasta"
    output:
        "{path}.fasta.fai"
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools faidx {input}"

# create a dict file for a fasta file
rule create_dict:
    input:
        "{path}.fasta"
    output:
        "{path}.dict"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk CreateSequenceDictionary R={input} O={output}"

# filter VCF by sample set
rule filter_sample_set:
    input:
        vcf="../resources/genome/betula/all.vcf.gz",
        tbi="../resources/genome/betula/all.vcf.gz.tbi",
        ref="../resources/genome/betula/genome.fasta",
        index="../resources/genome/betula/genome.fasta.fai",
        dict="../resources/genome/betula/genome.dict",
        sample_set="../resources/genome/betula/sample_sets/{sample_set}.args"
    output:
        r"results/vcf/betula/{sample_set,\w+}.vcf.gz"
    params:
        command="SelectVariants",
        flags=lambda w: {
            "-R": "../resources/genome/betula/genome.fasta",
            "-V": "../resources/genome/betula/all.vcf.gz",
            "--sample-name": f"../resources/genome/betula/sample_sets/{w.sample_set}.args",
            "--remove-unused-alternates": True
        }
    conda:
        "envs/gatk.yaml"
    script:
        "scripts/run_gatk.py"

# get the interval lists of the chunks to be created
rule split_intervals:
    input:
        ref="../resources/genome/betula/genome.fasta",
        index="../resources/genome/betula/genome.fasta.fai",
        vcf="results/vcf/betula/{sample_set}.vcf.gz",
        tbi="results/vcf/betula/{sample_set}.vcf.gz.tbi"
    output:
        [f"results/vcf/betula/{{sample_set}}/interval_lists/{str(i).zfill(4)}-scattered.interval_list" for i
         in range(config['n_chunks_vcf'])]
    params:
        n=config['n_chunks_vcf'],
        out_prefix='results/vcf/betula/{sample_set}/interval_lists'
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk SplitIntervals -R {input.ref} -L {input.vcf} --scatter-count {params.n} -O {params.out_prefix}"

# split the VCF file into chunks
rule split_vcf:
    input:
        vcf="results/vcf/betula/{sample_set}.vcf.gz",
        tbi="results/vcf/betula/{sample_set}.vcf.gz.tbi",
        ref="../resources/genome/betula/genome.fasta",
        index="../resources/genome/betula/genome.fasta.fai",
        dict="../resources/genome/betula/genome.dict",
        list=lambda w: f"results/vcf/betula/{{sample_set}}/interval_lists/{str(w.i).zfill(4)}-scattered.interval_list"
    output:
        "results/vcf/betula/{sample_set}/vcf/{i}.vcf.gz"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk SelectVariants -R {input.ref} -V {input.vcf} -L {input.list} -O {output}"

# parse SFS from VCF
rule parse_vcf:
    input:
        vcf="results/vcf/betula/{sample_set}/vcf/{i}.vcf.gz",
        ref="../resources/genome/betula/genome.fasta"
    output:
        r"results/spectra/betula/{sample_set}/{stratifications}/{n}/{i,\d+}.csv"
    params:
        n=lambda w: int(w.n),
        stratifications=lambda w: w.stratifications.split('.')
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf.py"

# parse SFS from VCF
rule merge_spectra:
    input:
        lambda w: expand("results/spectra/betula/{sample_set}/{stratifications}/{n}/{i}.csv",i=range(
            config['n_chunks_vcf']),allow_missing=True)
    output:
        "results/spectra/betula/{sample_set}/{stratifications}/{n}/all.csv"
    conda:
        "envs/base.yaml"
    script:
        "scripts/merge_spectra.py"

# run joint inference from template
rule run_joint_inference_from_templates:
    input:
        "../resources/configs/shared/{config}/config.yaml"
    output:
        summary="results/fastdfe/templates/shared/{config}/summary.json",
        serialized="results/fastdfe/templates/shared/{config}/serialized.json"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_shared_dfe.py"

# load EST-SFS module
module est_sfs:
    snakefile:
        "rules/est-sfs.smk"
    config:
        {
            'vcf_in': "../resources/genome/betula/all.with_outgroups.subset.10000.vcf.gz",
            'vcf_out': "../resources/genome/betula/all.with_outgroups.subset.10000.polarized.vcf.gz",
            'wildcards': {},
            'ingroups': "../resources/genome/betula/sample_sets/birch.args",
            'outgroups': "../resources/genome/betula/sample_sets/outgroups.args",
            'basepath': "results/est-sfs/",
            'basepath_vcf': "results/est-sfs/",
            'max_sites': 1000000,
            'n_samples': 50,
            'n_outgroup': 3,
            'model': 1,
            'nrandom': 10,
            'log_level': 20
        }

# load all rules from EST-SFS module
use rule * from est_sfs as *

# annotate synonymy
rule annotate_synonymy:
    input:
        vcf="../resources/genome/{species}/{name}.vcf.gz",
        ref="../resources/genome/{species}/genome.fasta",
        gff="../resources/genome/{species}/genome.gff.gz"
    output:
        "results/vcf/{species}/{name}.synonymy.vcf.gz"
    conda:
        "envs/base.yaml"
    script:
        "scripts/annotate_synonymy.py"

# annotate degeneracy
rule annotate_degeneracy:
    input:
        vcf="../resources/genome/{species}/{name}.vcf.gz",
        ref="../resources/genome/{species}/genome.fasta",
        gff="../resources/genome/{species}/genome.gff.gz"
    output:
        "results/vcf/{species}/{name}.degeneracy.vcf.gz"
    conda:
        "envs/base.yaml"
    script:
        "scripts/annotate_degeneracy.py"

# predict the variants' effects with VEP
rule annotate_vep_betula:
    input:
        vcf="../resources/genome/betula/{name}.vcf.gz",
        ref="../resources/genome/betula/genome.fasta",
        gff="../resources/genome/betula/genome.gff.gz",
        tbi="../resources/genome/betula/genome.gff.gz.tbi"
    output:
        "results/vcf/betula/{name}.vep.vcf.gz",
        "results/vcf/betula/{name}.vep.vcf.gz_summary.html"
    params:
        species="betula"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# download human chromosome-level VCF
rule download_vcf_chr_sapiens:
    output:
        "../resources/genome/sapiens/{chr}.vcf.gz"
    params:
        url="https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/"
            "20181203_biallelic_SNV/ALL.{chr}.shapeit2_integrated_v1a.GRCh38.20181129.phased.vcf.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download human chromosome-level FASTA
rule download_fasta_chr_sapiens:
    output:
        "../resources/genome/sapiens/{chr}.fasta.gz"
    params:
        url="https://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/{chr}.fa.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download human chromosome-level GFF
rule download_gff_chr_sapiens:
    output:
        r"../resources/genome/sapiens/chr{name,\d+}.gff3.gz"
    params:
        url="http://ftp.ensembl.org/pub/release-109/gff3/homo_sapiens/Homo_sapiens.GRCh38.109.chromosome.{name}.gff3.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# sort GFF
rule sort_and_compress_gff:
    input:
        "{path}.{ext}.gz"
    output:
        "{path}.sorted.{ext,gtf|gff|gff3}"
    conda:
        "envs/tabix.yaml"
    shell:
        """
        zgrep '^#' {input} > {output}
        zgrep -v '^#' {input} | sort -V -k1,1 -k4,4n >> {output}
        """

# Sort and recompress GFF
rule sort_and_recompress_gff:
    input:
        "{path}.{ext}.gz"
    output:
        "{path}.corrected.{ext,gtf|gff|gff3}.gz"
    conda:
        "envs/tabix.yaml"
    shell:
        """
        (zcat < {input} | grep '^#' ; zcat < {input} | grep -v '^#' | sort -V -k1,1 -k4,4n) | bgzip -c > {output}
        """

# decompress fasta file
rule decompress_bgzip_fasta:
    input:
        "{path}.fasta.gz"
    output:
        "{path,.*}.fasta"
    conda:
        "envs/tabix.yaml"
    shell:
        "bgzip {input} -cd > {output}"

# recompress FASTA file
rule recompress_bgzip_fasta:
    input:
        "{path}.fasta.gz"
    output:
        "{path}.recompressed.fasta.gz"
    conda:
        "envs/tabix.yaml"
    shell:
        "gzip -cd {input} | bgzip -c > {output}"


# predict the variants' effects with VEP
rule annotate_vep_sapiens:
    input:
        vcf="../resources/genome/sapiens/{name}.vcf.gz",
        ref="../resources/genome/sapiens/{name}.fasta",
        gff="../resources/genome/sapiens/{name}.sorted.gff3.gz",
        tbi="../resources/genome/sapiens/{name}.sorted.gff3.gz.tbi"
    output:
        "results/vcf/sapiens/{name,[a-z0-9]+}.vep.vcf.gz",
        "results/vcf/sapiens/{name}.vep.vcf.gz_summary.html"
    params:
        species="homo_sapiens"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# predict the variants' effects with VEP
rule annotate_vep_sapiens_test:
    input:
        vcf="../resources/genome/sapiens/{name}_test.vcf.gz",
        ref="../resources/genome/sapiens/{name}.fasta",
        gff="../resources/genome/sapiens/{name}.sorted.gff3.gz",
        tbi="../resources/genome/sapiens/{name}.sorted.gff3.gz.tbi"
    output:
        "results/vcf/sapiens/{name}_test.vep.vcf.gz",
        "results/vcf/sapiens/{name}_test.vep.vcf.gz_summary.html"
    params:
        species="homo_sapiens"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# predict the variants' effects with SnpEff
rule annotate_snpeff_sapiens:
    input:
        vcf="../resources/genome/sapiens/{name}.vcf.gz"
    output:
        "results/vcf/sapiens/{name}.snpeff.vcf.gz"
    params:
        species="hg38"  # The name of the SnpEff database for the species
    conda:
        "envs/snpeff.yaml"
    script:
        "scripts/annotate_snpeff.py"

# generate a requirements.txt using poetry
rule generate_requirements_poetry:
    output:
        base="envs/requirements.txt",
        base_snakemake=".snakemake/conda/requirements.txt",
        testing="envs/requirements_testing.txt",
        testing_snakemake=".snakemake/conda/requirements_testing.txt",
        docs="../docs/requirements.txt"
    conda:
        "envs/build.yaml"
    shell:
        """
            poetry update
            poetry export -f requirements.txt --without-hashes -o {output.base}
            poetry export -f requirements.txt --without-hashes -o {output.base_snakemake}
            poetry export --with dev -f requirements.txt --without-hashes -o {output.testing}
            poetry export --with dev -f requirements.txt --without-hashes -o {output.testing_snakemake}
            poetry export --with dev -f requirements.txt --without-hashes -o {output.docs}
            mamba env update -f envs/dev.yaml
            mamba env update -f envs/base.yaml
        """

# download human chromosome-level VCF
rule download_vcf_hgdp:
    output:
        "results/vcf/hgdp/{chr}/opts.vcf.gz"
    params:
        url="https://ngs.sanger.ac.uk//production/hgdp/hgdp_wgs.20190516/hgdp_wgs.20190516.full.chr{chr}.vcf.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# create test sets ofr HGDP VCFs
rule subset_vcf_hgdp:
    input:
        "results/vcf/hgdp/{chr}/{opts}.vcf.gz"
    output:
        "results/vcf/hgdp/{chr}/{opts}.subset.{n}.vcf.gz"
    params:
        n=lambda w: int(w.n)
    conda:
        "envs/tabix.yaml"
    shell:
        """
        (zcat < {input} | grep "^#" ; zcat < {input} | grep -v "^#" | head -n {params.n} || true) | bgzip -c > {output}
        """

# download human chromosome-level FASTA
rule download_fasta_hgdp:
    output:
        "results/fasta/hgdp/{chr}.fasta.gz"
    params:
        url="http://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.{chr}.fa.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download human chromosome-level GFF
rule download_gff_hgdp:
    output:
        "results/gff/hgdp/{chr}.gff3.gz"
    params:
        url="http://ftp.ensembl.org/pub/release-109/gff3/homo_sapiens/Homo_sapiens.GRCh38.109.chromosome.{chr}.gff3.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download population metadata for HGDP
rule download_hgdp_population_metadata:
    output:
        "results/sample_sets/hgdp/all.csv"
    params:
        url="https://ngs.sanger.ac.uk//production/hgdp/hgdp_wgs.20190516/metadata/hgdp_wgs.20190516.metadata.txt"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# predict the variants' effects with VEP
rule annotate_vep_hgdp:
    input:
        vcf="results/vcf/hgdp/{chr}/{opts}.vcf.gz",
        ref="results/fasta/hgdp/{chr}.recompressed.fasta.gz",
        gff="results/gff/hgdp/{chr}.corrected.gff3.gz",
        tbi="results/gff/hgdp/{chr}.corrected.gff3.gz.tbi"
    output:
        "results/vcf/hgdp/{chr}/{opts}.vep.vcf.gz",
        "results/vcf/hgdp/{chr}/{opts}.vep.vcf.gz_summary.html"
    params:
        species="homo_sapiens"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# convert VCF file to PLINK format
rule convert_plink_sample_set_hgdp:
    input:
        "results/vcf/hgdp/{chr}/{opts}.vcf.gz"
    output:
        multiext("results/plink/hgdp/{chr}/{opts}.",'bed','bim','fam','log')
    conda:
        "envs/plink.yaml"
    script:
        "scripts/convert_plink.py"

# subset the given sample set by the 'val' in 'col'
# of the given CSV file
rule filter_sample_set_hgdp:
    input:
        "results/sample_sets/hgdp/all.csv"
    output:
        "results/sample_sets/hgdp/{population}.csv"
    conda:
        "envs/base.yaml"
    params:
        filter_col='population',
        filter_val='{population}'
    script:
        "scripts/filter_sample_set.py"

# create a list of sample names from the samples file
rule derive_sample_list_hgdp:
    input:
        "results/sample_sets/hgdp/{population}.csv"
    output:
        "results/sample_lists/hgdp/{population}.args"
    params:
        name_col='sample'
    conda:
        "envs/base.yaml"
    script:
        "scripts/derive_sample_list.py"

# generate PCA plot
rule plot_pca_hgdp:
    input:
        bed="results/plink/hgdp/{chr}/{opts}.bed",
        samples="results/sample_sets/hgdp/{population}.csv"
    output:
        "results/graphs/pca/hgdp/{chr}/{opts}/{population}.png"
    params:
        marker_size=15,
        name_col='sample',
        label_col='population',
        cbar=False,
        legend=True,
        subsample=10000
    conda:
        "envs/pca.yaml"
    script:
        "scripts/plot_pca.py"

# parse spectra from human chromosome-level VCF
rule parse_vcf_hgdp:
    input:
        vcf="results/vcf/hgdp/{chr}/{opts}.vcf.gz",
        fasta="results/fasta/hgdp/{chr}.fasta.gz",
        gff="results/gff/hgdp/{chr}.corrected.gff3.gz",
        samples="results/sample_lists/hgdp/{population}.args"
    output:
        csv=r"results/spectra/hgdp/{chr,\d+}/{opts}.n.{n}/{population}.csv",
        png=r"results/graphs/spectra/hgdp/{chr,\d+}/{opts}.n.{n}/{population}.png"
    params:
        aliases=lambda w: {f"chr{w.chr}": [f"{w.chr}"]},
        n=lambda w: int(w.n),
        chr="{chr}"
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf_hgdp.py"

# merge spectra for hgdp
rule merge_spectra_hgdp:
    input:
        expand("results/spectra/hgdp/{chr}/{opts}.n.{n}/{population}.csv",
            chr=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],allow_missing=True)
    output:
        "results/spectra/hgdp/all/{opts}.n.{n}/{population}.csv"
    conda:
        "envs/base.yaml"
    script:
        "scripts/merge_spectra.py"

# plot SFS for hgdp
rule plot_spectra_hgdp:
    input:
        "results/spectra/hgdp/all/{opts}.n.{n}/{population}.csv"
    output:
        "results/graphs/spectra/hgdp/all/{opts}.n.{n}/{population}.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/plot_spectra.py"

# prepare config file for HGDP
rule prepare_config_hgdp:
    input:
        config="../resources/configs/HGDP/{tool}/{config}.yaml",
        spectra="results/spectra/hgdp/{chr}/{opts}.n.{n}/{population}.csv"
    output:
        "results/{tool}/hgdp/config/{chr}/{opts}.n.{n}.{config}/{population}.yaml"
    conda:
        "envs/base.yaml"
    script:
        "scripts/prepare_config_hgdp.py"

# infer DFE from spectra for the HGDP data set using fastDFE
rule infer_dfe_hgdp_fastdfe:
    input:
        "results/fastdfe/hgdp/config/{chr}/{opts}.n.{n}.{config}/{population}.yaml"
    output:
        serialized="results/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        summary="results/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.summary.json",
        dfe="results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.dfe.png",
        spectra="results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.spectra.png",
        params="results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe.py"

# infer DFE from spectra for the HGDP data set using polyDFE
rule infer_dfe_hgdp_polydfe:
    input:
        "results/polydfe/hgdp/config/{chr}/{opts}.n.{n}.{config}/{population}.yaml"
    output:
        serialized="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        summary="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.summary.json",
        polydfe="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.polydfe.txt"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# create bootstrap samples for polyDFE
rule create_bootstrap_hgdp_polydfe:
    input:
        "results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        config['polydfe_postprocessing_source']
    output:
        "results/polydfe/hgdp/config/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.yaml"
    conda:
        dev_env
    script:
        "scripts/create_bootstrap_polydfe.py"

# infer DFE from bootstrapped SFS using polyDFE
rule infer_dfe_hgdp_polyDFE_bootstrap:
    input:
        "results/polydfe/hgdp/config/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.yaml",
        config['polydfe_postprocessing_source']
    output:
        polydfe="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.polydfe.txt",
        summary="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.summary.json",
        serialized="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.serialized.json"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# merge bootstrap samples for polyDFE
rule merge_bootstraps_hgdp_polydfe:
    input:
        config['polydfe_postprocessing_source'],
        original="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        bootstraps=lambda w: expand("results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.serialized.json",i=range(
            config['n_bootstraps_hgdp']),allow_missing=True)
    output:
        "results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{population}.serialized.json"
    conda:
        dev_env
    script:
        "scripts/merge_bootstraps_polydfe.py"

# plot polyDFE inference results
rule plot_polydfe_inference_hgdp:
    input:
        "results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{population}.serialized.json"
    output:
        dfe_discretized="results/graphs/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.dfe.png",
        mle_params="results/graphs/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.params.png"
    conda:
        dev_env
    script:
        "scripts/plot_polydfe_inference.py"

# download drosophila data from https://doi.org/10.1371/journal.pbio.3001775
rule download_sfs_covariate_tables_drosophila:
    output:
        "results/sfs_covariates/drosophila.csv"
    params:
        url="https://gitlab.gwdg.de/molsysevol/supplementarydata_geneage/-/raw/master/Data/S31_Data.csv"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download arabidopsis data from https://doi.org/10.1371/journal.pbio.3001775
rule download_sfs_covariate_tables_arabidopsis:
    output:
        "results/sfs_covariates/arabidopsis.csv"
    params:
        url="https://gitlab.gwdg.de/molsysevol/supplementarydata_geneage/-/raw/master/Data/S32_Data.csv"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# prepare config file for hominidae
rule prepare_config_hominidae:
    input:
        neutral="../resources/SFS/hominidae/uSFS/{type}_4fold_all_sfs.txt",
        selected="../resources/SFS/hominidae/uSFS/{type}_0fold_all_sfs.txt"
    output:
        "results/fastdfe/hominidae/{type}.yaml"
    conda:
        "envs/base.yaml"
    script:
        "scripts/prepare_config_hominidae.py"

# infer DFE from spectra for the hominidae data set using fastDFE
rule infer_dfe_hominidae_fastdfe:
    input:
        "results/fastdfe/hominidae/{type}.yaml"
    output:
        serialized="results/fastdfe/hominidae/{type,[a-zA-Z]}.serialized.json",
        summary="results/fastdfe/hominidae/{type,[a-zA-Z]}.summary.json",
        dfe="results/graphs/fastdfe/hominidae/{type,[a-zA-Z]}.dfe.png",
        spectra="results/graphs/fastdfe/hominidae/{type,[a-zA-Z]}.spectra.png",
        params="results/graphs/fastdfe/hominidae/{type,[a-zA-Z]}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe.py"

# create config files for checking covariates
rule create_config_covariates_hominidae:
    input:
        neutral=expand('../resources/SFS/hominidae/uSFS/{t}_4fold_all_sfs.txt',t=hominidae),
        selected=expand('../resources/SFS/hominidae/uSFS/{t}_0fold_all_sfs.txt',t=hominidae)
    output:
        "results/fastdfe/hominidae/cov/{param}.yaml"
    params:
        param="{param}",
        types=hominidae
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config_covariates_hominidae.py"

# run joint inference using covariates
rule infer_dfe_hominidae_covariates:
    input:
        "results/fastdfe/hominidae/cov/{param}.yaml"
    output:
        serialized="results/fastdfe/hominidae/cov/{param}.serialized.json",
        summary="results/fastdfe/hominidae/cov/{param}.summary.json",
        dfe="results/graphs/fastdfe/hominidae/cov/{param}.dfe.png",
        spectra="results/graphs/fastdfe/hominidae/cov/{param}.spectra.png",
        params="results/graphs/fastdfe/hominidae/cov/{param}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe_covariates.py"

# parse betula SFS which are already polarized
rule parse_vcf_betula:
    input:
        vcf='../resources/genome/betula/all.polarized.vcf.gz',
        ref='../resources/genome/betula/genome.fasta',
        gff='../resources/genome/betula/genome.gff.gz',
        samples='../resources/genome/betula/sample_sets/{sample_set}.args'
    output:
        csv="results/sfs/betula/{sample_set}/{n}/sfs.csv",
        png="results/graphs/sfs/betula/{sample_set}/{n}/sfs.png"
    params:
        n=lambda w: int(w.n)
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf_betula.py"

# parse betula SFS, and polarize them on the fly
rule parse_polarize_vcf_betula:
    input:
        vcf='../resources/genome/betula/all.vcf.gz',
        ref='../resources/genome/betula/genome.fasta',
        gff='../resources/genome/betula/genome.gff.gz',
        samples='../resources/genome/betula/sample_sets/{sample_set}.args'
    output:
        csv="results/sfs/betula/{sample_set}/{n}/sfs.polarized-on-the-fly.csv",
        png="results/graphs/sfs/betula/{sample_set}/{n}/sfs.polarized-on-the-fly.png"
    params:
        n=lambda w: int(w.n)
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf_betula.py"

# create config files for checking covariates
rule infer_dfe_covariates_arabidopsis:
    input:
        "results/sfs_covariates/arabidopsis.csv"
    output:
        "results/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.yaml"
    params:
        param="{param}",
        col="{col}",
        n_bins=lambda w: int(w.n_bins),
        subsample_size=lambda w: int(w.subsample_size)
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config_covariates_arabidopsis.py"

# run joint inference using covariates
rule infer_dfe_arabidopsis_covariates:
    input:
        "results/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.yaml"
    output:
        serialized="results/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.serialized.json",
        summary="results/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.summary.json",
        dfe="results/graphs/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.dfe.png",
        covariates="results/graphs/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.covariates.png",
        spectra="results/graphs/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.spectra.png",
        params="results/graphs/fastdfe/arabidopsis/cov/{param}/{col}/{n_bins}.{subsample_size}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe_covariates.py"

# annotate polarization
rule annotation_polarization_betula:
    input:
        vcf="../resources/genome/betula/{name}.vcf.gz"
    output:
        "results/vcf/betula/{name}.polarized.vcf.gz"
    params:
        outgroups=["ERR2103730"],
        n_ingroups=20,
        exclude=["ERR2103731"],
        confidence_threshold=0
    conda:
        "envs/base.yaml"
    script:
        "scripts/annotate_polarization.py"

module subset_vcf:
    snakefile:
        "rules/subset_vcf.smk"

use rule * from subset_vcf

# simulate trees using SLiM
rule simulate_trees_slim:
    output:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/chunk={chunk}/{demography}/sequence.trees"
    params:
        g=lambda w: int(float(w.g)),
        L=lambda w: int(float(w.L)),
        mu=lambda w: float(w.mu),
        r=lambda w: float(w.r),
        N=lambda w: int(float(w.N)),
        s_b=lambda w: float(w.s_b),
        b=lambda w: float(w.b),
        s_d=lambda w: float(w.s_d),
        p_b=lambda w: float(w.p_b),
        demography="{demography}"
    conda:
        "envs/slim.yaml"
    shell:
        """
        slim \
        -d "out=\'{output}\'" \
        -d "g={params.g}" \
        -d "length={params.L}" \
        -d "mu={params.mu}" \
        -d "r={params.r}" \
        -d "N={params.N}" \
        -d "s_b={params.s_b}" \
        -d "b={params.b}" \
        -d "s_d={params.s_d}" \
        -d "p_b={params.p_b}" \
        -d "demography=\'{params.demography}\'" \
        scripts/simulate_trees.slim
        """

# parse SLiM trees
rule compute_sfs_slim:
    input:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/chunk={chunk}/{demography}/sequence.trees"
    output:
        sfs="results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/chunk={chunk}/{demography}/unfolded/sfs.csv",
        ds_continuous="results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/chunk={chunk}/{demography}/s.simulated.continuous.png",
        ds_discretized="results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/chunk={chunk}/{demography}/s.simulated.discretized.png"
    params:
        n=lambda w: int(w.n),
        L=lambda w: int(float(w.L)),
        mu=lambda w: float(w.mu),
        N=lambda w: int(float(w.N)),
        s_b=lambda w: float(w.s_b),
        b=lambda w: float(w.b),
        s_d=lambda w: float(w.s_d),
        p_b=lambda w: float(w.p_b)
    conda:
        "envs/slim.yaml"
    script:
        "scripts/compute_sfs_slim.py"

# fold SFS
rule fold_sfs_slim:
    input:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/unfolded/sfs.csv"
    output:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/folded/sfs.csv"
    conda:
        "envs/base.yaml"
    script:
        "scripts/fold_sfs.py"

# merge chunks
rule merge_chunks_sfs_slim:
    input:
        lambda w: expand("results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/chunk={chunk}/{demography}/{folded}/sfs.csv",chunk=range(int(w.n_chunks)),allow_missing=True)
    output:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/sfs.csv"
    conda:
        "envs/base.yaml"
    script:
        "scripts/merge_spectra.py"

# plot DFE simulated using SLiM
rule plot_dfe_slim:
    input:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/sfs.csv"
    output:
        "results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/dfe.simulated.png"
    params:
        mu=lambda w: float(w.mu),
        s_b=lambda w: float(w.s_b),
        b=lambda w: float(w.b),
        s_d=lambda w: float(w.s_d),
        p_b=lambda w: float(w.p_b)
    conda:
        "envs/base.yaml"
    script:
        "scripts/plot_dfe_slim.py"

# infer DFE using fastdfe from SLiM trees
rule infer_dfe_slim_fastdfe:
    input:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/sfs.csv"
    output:
        serialized="results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/serialized.json",
        summary="results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/summary.json",
        dfe="results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/dfe.inferred.png",
        model_fit="results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/model_fit.png",
        spectra="results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/spectra.png",
        params="results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/params.png"
    params:
        full_dfe=lambda w: w.p_b != 0
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe_slim_fastdfe.py"

# simulate SFS using fastDFE
rule simulate_sfs:
    input:
        "results/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/sfs.csv"
    output:
        sfs="results/sfs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/sfs.csv",
        comp="results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/sfs.png"
    params:
        mu=lambda w: float(w.mu),
        s_b=lambda w: float(w.s_b),
        b=lambda w: float(w.b),
        s_d=lambda w: float(w.s_d),
        p_b=lambda w: float(w.p_b),
        n=lambda w: int(w.n),
        #title=lambda w: f"g={w.g}/L={int(float(w.L) * int(w.n_chunks)):.1e}/mu={w.mu}/r={w.r}/N={w.N}/s_b={w.s_b}/b={w.b}/s_d={w.s_d}/p_b={w.p_b}"
        title=lambda w: f"$s_b=${w.s_b}, $b=${w.b}, $s_d=${w.s_d}, $p_b=${w.p_b}"
    conda:
        "envs/dev.yaml"
    script:
        "scripts/simulate_sfs.py"

# plot SFS comparison collage
rule combine_plots_collage_slim:
    input:
        lambda w: expand(collages[w.collage_type], **w)
    output:
        "results/graphs/comp/slim/n_replicate={n_replicate}/g={g}/N={N}/r={r}/n={n}/{demography}/{folded}/collage/{collage_type}.{plot_type}.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# combine DFE plots
rule combine_dfe_plots_slim:
    input:
        "results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/dfe.simulated.png",
        "results/graphs/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/dfe.inferred.png"
    output:
        "results/graphs/comp/slim/n_replicate={n_replicate}/n_chunks={n_chunks}/g={g}/L={L}/mu={mu}/r={r}/N={N}/s_b={s_b}/b={b}/s_d={s_d}/p_b={p_b}/n={n}/{demography}/{folded}/dfe.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# update the documentation
rule update_docs:
    output:
        directory("../docs/_build")
    conda:
        "envs/dev.yaml"
    shell:
        "make html -C ../docs"

# plot SFS for hgdp
rule plot_spectra:
    input:
        "results/{path}/sfs.csv"
    output:
        "results/graphs/spectra/{path}/sfs.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/plot_spectra.py"
