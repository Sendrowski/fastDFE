import re
from typing import List

configfile: "config.yaml"

# whether we run snakemake locally
is_local = config.get('local',False)

# currently we are using macOS locally
is_macos = is_local

# whether we are using macOS
is_macos = config.get('macos',is_macos)

if is_macos:
    # we installed it locally
    dev_env = 'envs/dev.yaml'
else:
    # this only works on Linux
    dev_env = 'envs/dev_with_polydfe.yaml'


def extract_opt(str: str, name, default_value=None) -> str:
    """
    Extract named option from string like 'opt1_5.opt2_test' wh

    :param str: String to extract from
    :param name: Name of option
    :param default_value: Default value if option is not found
    :return: Extracted option or default value
    """
    # named options have the following signature
    match = re.search(f"[./-]{name}[_:]([^./-]*)",str)

    if match:
        return match.groups()[0]

    return default_value


def expand_comparisons(path) -> (List[str], List[str]):
    """
    Expand over the comma separated lists in 'path'

    :param path: Path to expand
    :return: Expanded paths and labels
    """
    matches = re.finditer("\[(.*?)\]",path)

    def expand(strings: List[str], match: re.Match) -> (List[str], List[str]):
        """
        Expand by substituting first match in given strings.
        Also attempt to parse labels enclosed in <> tags.

        :param strings: Strings to expand
        :param match: Match to expand with
        :return: Expanded strings and labels
        """
        elements = match.groups()[0].split(',')

        expanded, labels = [], []
        for string in strings:
            for e in elements:
                # attempt to match explicit label
                if matched_name := re.search("<(.*)>",e):
                    # remove label tags
                    e = re.sub("<.*>",'',e)

                    label = matched_name.groups()[0]
                else:
                    label = None

                expanded.append(re.sub(match.re.pattern,e,string,1))
                labels.append(label)

        return expanded, labels

    expanded = [path]
    labels = []
    for match in matches:
        expanded, labels = expand(expanded,match)

    # set labels to None if first one evaluates to false
    if not labels[0]:
        labels = None

    # prepend base path to strings
    return ["results/graphs/" + s for s in expanded], labels


configs = expand(
    "{species}_{model}_{type}",
    species=['pendula', 'pubescens', 'example_1', 'example_2', 'example_3'],
    model=['C'],
    type=['full', 'full_anc', 'deleterious', 'deleterious_anc']
)

# model mapping
polydfe_models = {
    'A': 'DisplacedGammaParametrization',
    'B': 'GammaDiscreteParametrization',
    'C': 'GammaExpParametrization',
    'D': 'DiscreteParametrization'
}

ruleorder:
    merge_bootstraps_polyDFE > infer_dfe_polyDFE >
    create_config > create_config_from_spectra >

wildcard_constraints:
    opts=r'[^/]*',# match several optional options not separated by /
    chr='\w+',
    pop="[a-zA-Z]+",
    param="[a-zA-Z0-9_]+",
    config='\w+',
    population='\w+',

hominidae = [
    'bonobo',
    'bornean_orang',
    'central_chimp',
    'eastern_chimp',
    'human',
    'NC_chimp',
    'sumatran_orang',
    'western_chimp',
    'western_lowland_gorilla'
]

rule all:
    input:
        (
            #expand("results/graphs/fastdfe/arabidopsis/cov/{param}.{col}.params.png", param=['S_d'], col=['age.rsa']),
            #expand("results/sfs/betula/{sample_set}/{n}/sfs.csv", sample_set=['pendula', 'pubescens'], n=[20]),
            #expand("results/graphs/comp/spectra/hgdp/inference/{chr}/{opts}/[all,Japanese].png",chr=[21, 1], opts=['opts.subset.50000.n.10', 'opts.subset.50000.n.20']),
            #expand("results/graphs/comp/pca/hgdp/inference/{chr}/{opts}/[all,Japanese].png",chr=[21], opts=['opts.subset.50000']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[10,20].config.{config}/[all,Japanese].dfe.png",chr=[1, 2, 3], opts=['opts'], config=['default']),
            #expand("results/graphs/comp/[fastdfe,polydfe]/hgdp/inference/{chr}/{opts}.n.10.config.{config}/[all].dfe.png",chr=[1, 2, 21, 22],opts=['opts'],config=['default']),
            #expand("results/graphs/comp/[fastdfe,polydfe]/hgdp/inference/{chr}/{opts}.n.[5,10,15,20,25,30,35,40].config.{config}/[all].dfe.png",chr=[1, 2, 3, 'all'],opts=['opts'],config=['default']),
            expand("results/graphs/comp/spectra/hgdp/{chr}/{opts}.n.[5,10,15,20,25,30,35,40]/[all].png",chr=[1, 2, 3, 4, 5, 'all'],opts=['opts'],config=['default']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[5,10,15,20,25,30,35,40].config.{config}/[all].dfe.png",chr=[1, 2, 3, 'all'],opts=['opts'],config=['deleterious']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[5,10,15,20,25,30,35,40].config.{config}/[all].dfe.png",chr=[1],opts=['opts'],config=['deleterious']),
            #expand("results/graphs/comp/fastdfe/hgdp/inference/{chr}/{opts}.n.[10].config.{config}/[all].dfe.png",chr=[8, 9],opts=['opts'],config=['deleterious']),
            #expand("results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.10.config.{config}/all.dfe.png",chr=[21],opts=['opts.vep'],config=['default']),
            #expand("results/graphs/comp/fastdfe/hominidae/[{types}].dfe.png",types=','.join(hominidae)),
            #expand("results/graphs/comp/fastdfe/hominidae/[{types}].params.png",types=','.join(hominidae)),
            #expand("results/graphs/fastdfe/hominidae/cov/{param}.params.png", param=['S_d', 'eps', 'p_b', 'S_b', 'b']),
            #expand("results/graphs/comp/fastdfe/hominidae/cov/[S_d,eps,p_b,S_b,b].params.png"),
            #expand("results/graphs/comp/fastdfe/hominidae/cov/[S_d,eps,p_b,S_b,b].dfe.png")
            #expand("results/graphs/comp/fastdfe/hgdp/{chr}/{opts}.subset.50000.n.[10,20].config.{config}/[all,Japanese].dfe.png",chr=[21], opts=['opts'], config=['test']),
            #expand("results/configs/{config}_bootstrapped_100/config.yaml",config=configs),
            #expand("results/fastdfe/{config}_bootstrapped_100/serialized.json",config=configs),
            #expand("results/configs/{config}/config.yaml",config=['pendula.pubescens.example_1.example_2.example_3_C_full_anc']),
            #expand("results/graphs/fastdfe/{config}_bootstrapped_100/dfe_discretized.png",config=configs),
            #expand("results/spectra/{species}.csv", species=['pendula.pubescens', 'pendula.pubescens.example_1.example_2.example_3'])
            #expand("results/polydfe/{config}_bootstrapped_100/serialized.json",config=configs),
            #expand("results/graphs/comp/[fastdfe<fastDFE>,polydfe<polyDFE>]/{config}_bootstrapped_100/{type}.png",
            #    config=configs,
            #    type=['mle_params', 'dfe_discretized']
            #),
            #expand("results/graphs/comp/[fastdfe<fastDFE>,polydfe<polyDFE>]/{config}/{type}.png",
            #    config=['pendula_C_full'],
            #    type=['mle_params', 'dfe_discretized']
            #),
            #expand("results/graphs/{tool}/{config}/sfs_comparison.png",config=configs, tool=['fastdfe', 'polydfe']),
            #expand("results/graphs/comp/tool_comparison_collage/{type}.png",type=['discretized', 'inferred_params']),
            #expand("results/fastdfe/{config}_bootstrapped_sequential_100/serialized.json",config=configs),
            #expand("results/spectra/betula/pendula/{s}/all.csv",
            #    s=[
            #        'DegeneracyStratification',
            #        'DegeneracyStratification.BaseContextStratification',
            #        'DegeneracyStratification.BaseTransitionStratification',
            #        'DegeneracyStratification.BaseContextStratification.BaseTransitionStratification'
            #    ]
            #)
            #expand(f"results/graphs/comp/tool_comp/[{','.join([f'{n}_bootstrapped_100' for n in configs])}]/discretized.png"),
            #expand("results/fastdfe/{species}_{model}_{type}/config.yaml",species=['pendula', 'pubescens'],model=['C'],type=['full', 'full_anc']),
            #expand("results/graphs/fastdfe/{config}/dfe_discretized.png",config=['pendula_C_full', 'pendula_C_full_anc', 'pubescens_C_full', 'pubescens_C_full_anc']),
            #expand("results/polydfe/{config}/bootstrapped_{n}/serialized.json",config=['pendula_C_full_anc'],n=10),

            # tests
            #expand("results/testing/{config}_bootstrapped_100/overlapping_confidence_intervals.txt",config=configs)
            #expand("results/graphs/comp/spectra/[default,vep,vep_biallelic,vep_biallelic_coding]/hgdp/1/opts.n.10/all.png")
            # "../docs/requirements.txt"
        )

# fetch the postprocessing script from the GitHub repository
rule fetch_postprocessing_script_polyDFE:
    output:
        config['polydfe_postprocessing_source']
    params:
        url="https://github.com/paula-tataru/polyDFE/raw/master/postprocessing.R"
    conda:
        "envs/wget.yaml"
    shell:
        "wget {params.url} -O {output}"

# create a configuration file for DFE inference
rule create_config:
    input:
        sfs="../resources/polydfe/{species}/spectra/sfs.txt",
        init="../resources/polydfe/init/{model}.{type}_init"
    output:
        "results/configs/{species,pendula|pubescens|example_1|example_2|example_3}_{model,[ABCD]}_{type,full_anc|full|deleterious|deleterious_anc}/config.yaml"
    params:
        model=lambda w: polydfe_models[w.model]
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config.py"

# modify an existing config file
rule modify_config_do_bootstrap:
    input:
        "results/configs/{config}/config.yaml"
    output:
        "results/configs/{config}_bootstrapped_{n,\d+}/config.yaml"
    params:
        opts=lambda w: dict(
            do_bootstrap=True,
            n_bootstraps=int(w.n)
        )
    conda:
        "envs/base.yaml"
    script:
        "scripts/modify_config.py"

# modify an existing config file
rule modify_config_do_bootstrap_sequential:
    input:
        "results/configs/{config}/config.yaml"
    output:
        "results/configs/{config}_bootstrapped_sequential_{n,\d+}/config.yaml"
    params:
        opts=lambda w: dict(
            do_bootstrap=True,
            n_bootstraps=int(w.n),
            parallelize=False
        )
    conda:
        "envs/base.yaml"
    script:
        "scripts/modify_config.py"

# modify an existing config file
rule modify_config_seeded:
    input:
        "results/configs/{config}/config.yaml"
    output:
        "results/configs/{config}_seeded_{seed}/config.yaml"
    params:
        opts=lambda w: dict(
            seed=int(w.seed)
        )
    conda:
        "envs/base.yaml"
    script:
        "scripts/modify_config.py"

# infer DFE from SFS
rule infer_dfe:
    input:
        "results/configs/{config}/config.yaml"
    output:
        summary="results/fastdfe/{config}/summary.json",
        serialized="results/fastdfe/{config}/serialized.json"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe.py"

# infer DFE from SFS using polyDFE
rule infer_dfe_polyDFE:
    input:
        "results/configs/{config}/config.yaml",
        config['polydfe_postprocessing_source']
    output:
        polydfe="results/polydfe/{config}/out.txt",
        summary="results/polydfe/{config}/summary.json",
        serialized="results/polydfe/{config}/serialized.json"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# infer DFE from bootstrapped SFS using polyDFE
rule infer_dfe_polyDFE_bootstrap:
    input:
        "results/polydfe/{config}/bootstraps/{i}/config.yaml",
        config['polydfe_postprocessing_source']
    output:
        polydfe="results/polydfe/{config}/bootstraps/{i}/out.txt",
        summary="results/polydfe/{config}/bootstraps/{i}/summary.json",
        serialized="results/polydfe/{config}/bootstraps/{i}/serialized.json"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# infer DFE from SFS
rule plot_DFE_inference:
    input:
        "results/fastdfe/{config}/serialized.json"
    output:
        dfe_discretized="results/graphs/fastdfe/{config}/dfe_discretized.png",
        dfe_log="results/graphs/fastdfe/{config}/fastdfe_log.png",
        sfs_comparison="results/graphs/fastdfe/{config}/sfs_comparison.png",
        mle_params="results/graphs/fastdfe/{config}/mle_params.png",
        bucket_sizes="results/graphs/fastdfe/{config}/bucket_sizes.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/plot_base_inference.py"

# create config file for bootstrapped samples to be run with polyDFE
rule create_bootstrap_polyDFE:
    input:
        "results/polydfe/{config}/serialized.json",
        config['polydfe_postprocessing_source']
    output:
        "results/polydfe/{config}/bootstraps/{i}/config.yaml"
    conda:
        dev_env
    script:
        "scripts/create_bootstrap_polydfe.py"

# merge bootstraps and original inference result
rule merge_bootstraps_polyDFE:
    input:
        config['polydfe_postprocessing_source'],
        original="results/polydfe/{config}/serialized.json",
        bootstraps=lambda w: expand("results/polydfe/{config}/bootstraps/{i}/serialized.json",
            i=range(int(w.n)),allow_missing=True)
    output:
        "results/polydfe/{config}_bootstrapped_{n}/serialized.json"
    conda:
        dev_env
    script:
        "scripts/merge_bootstraps_polydfe.py"

# infer DFE from SFS
rule plot_polyDFE_inference:
    input:
        "results/polydfe/{config}/serialized.json"
    output:
        dfe_discretized="results/graphs/polydfe/{config}/dfe_discretized.png",
        mle_params="results/graphs/polydfe/{config}/mle_params.png"
    conda:
        dev_env
    script:
        "scripts/plot_polydfe_inference.py"

# combine a number of plots in a new plot
# comma-separated strings are expanded
rule combine_plots:
    input:
        lambda w: expand_comparisons(w.path)[0]
    output:
        "results/graphs/comp/{path}"
    params:
        titles=lambda w: expand_comparisons(w.path)[1]
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# test whether the confidence intervals of the discretized DFEs overlap
rule test_overlapping_confidence_intervals:
    input:
        fastdfe='results/fastdfe/{config}/serialized.json',
        polydfe='results/polydfe/{config}/serialized.json'
    output:
        touch("results/testing/{config}/overlapping_confidence_intervals.txt")
    conda:
        dev_env
    script:
        "scripts/check_overlapping_confidence_intervals.py"

# copy to cache for testing
rule copy_to_testing:
    input:
        "results/{all}"
    output:
        "../testing/{all}"
    conda:
        "envs/base.yaml"
    shell:
        "cp -r {input} {output}"

# renew test cache used for comparison
rule renew_test_file_cache_fastdfe:
    input:
        "../testing/configs/pendula_C_full_anc/config.yaml",
        "../testing/configs/pendula.pubescens.example_1.example_2.example_3_C_full_anc/config.yaml",
        "../testing/fastdfe/pendula_C_full_anc/serialized.json",
        "../testing/fastdfe/templates/shared/shared_example_1/serialized.json",
        "../testing/fastdfe/templates/shared/shared_example_2/serialized.json",
        "../testing/fastdfe/templates/shared/covariates_dummy_example_1/serialized.json",
        "../testing/fastdfe/templates/shared/covariates_Sd_example_1/serialized.json"
    output:
        touch("../testing/renew_cache_fastdfe.txt")

# renew test cache used for comparison
rule renew_test_file_cache_polydfe:
    input:
        "../testing/polydfe/pendula_C_full_anc/serialized.json"
    output:
        touch("../testing/renew_cache_polydfe.txt")

# renew test cache used for comparison
rule renew_test_file_cache:
    input:
        "../testing/renew_cache_fastdfe.txt",
        "../testing/renew_cache_polydfe.txt"
    output:
        touch("../testing/renew_cache.txt")

# combine spectra from several species
rule merge_spectra_species:
    input:
        lambda w: expand("../resources/polydfe/{s}/spectra/sfs.txt",s=w.species.split('.'))
    output:
        "results/spectra/{species}.csv"
    params:
        names=lambda w: w.species.split('.')
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_spectra.py"

# create a configuration file from a serialized spectra object
rule create_config_from_spectra:
    input:
        spectra="results/spectra/{species}.csv",
        # take init file for first species
        init=lambda w: f"../resources/polydfe/init/{w.model}.{w.type}_init"
    output:
        "results/configs/{species}_{model,[ABCD]}_{type,full_anc|full|deleterious|deleterious_anc}/config.yaml"
    params:
        model=lambda w: polydfe_models[w.model]
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config_from_spectra.py"

# plot tool comparison
rule plot_tool_comparison:
    input:
        fastdfe='results/fastdfe/{config}/serialized.json',
        polydfe='results/polydfe/{config}/serialized.json'
    output:
        discretized='results/graphs/tool_comp/{config}/discretized.png',
        inferred_params='results/graphs/tool_comp/{config}/inferred_params.png'
    conda:
        dev_env
    script:
        "scripts/plot_tool_comparison.py"

# plot fastdfe/polydfe collage
# file names get too long when trying to use combine_plots
rule tool_comparison_collage:
    input:
        lambda w:
        expand_comparisons(f"tool_comp/[{','.join([f'{c}_bootstrapped_100' for c in configs])}]/{w.type}.png")[0]
    output:
        "results/graphs/comp/tool_comparison_collage/{type}.png"
    params:
        titles=lambda w: [c.replace('_C_','_').replace('_',' ') for c in configs],
        title_size_rel=50,
        n_cols=4,
        n_rows=5,
        figsize=(16, 15),
        dpi=200
    conda:
        "envs/base.yaml"
    script:
        "scripts/combine_plots.py"

# create a tbi index for a vcf file
rule create_tbi_vcf:
    input:
        "{path}.vcf.gz"
    output:
        "{path}.vcf.gz.tbi"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk IndexFeatureFile -I {input}"

# create a tbi index for a gff file
rule create_tbi_gff:
    input:
        "{path}.{ext}.gz"
    output:
        "{path}.{ext,gff|gtf|gff3}.gz.tbi"
    conda:
        "envs/tabix.yaml"
    shell:
        "tabix -p gff {input}"

# create a tbi index for a gff file
"""
rule bgzip_gff:
    input:
        "{path}.{ext}"
    output:
        "{path}.{ext,gff|gtf|gff3}.gz"
    conda:
        "envs/tabix.yaml"
    shell:
        "bgzip -c {input} > {input}.gz"
"""

# index a fasta file using samtools
rule create_fai:
    input:
        "{path}.fasta"
    output:
        "{path}.fasta.fai"
    conda:
        "envs/samtools.yaml"
    shell:
        "samtools faidx {input}"

# create a dict file for a fasta file
rule create_dict:
    input:
        "{path}.fasta"
    output:
        "{path}.dict"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk CreateSequenceDictionary R={input} O={output}"

# filter VCF by sample set
rule filter_sample_set:
    input:
        vcf="../resources/genome/betula/all.vcf.gz",
        tbi="../resources/genome/betula/all.vcf.gz.tbi",
        ref="../resources/genome/betula/genome.fasta",
        index="../resources/genome/betula/genome.fasta.fai",
        dict="../resources/genome/betula/genome.dict",
        sample_set="../resources/genome/betula/sample_sets/{sample_set}.args"
    output:
        "results/vcf/betula/{sample_set,\w+}.vcf.gz"
    params:
        command="SelectVariants",
        flags=lambda w: {
            "-R": "../resources/genome/betula/genome.fasta",
            "-V": "../resources/genome/betula/all.vcf.gz",
            "--sample-name": f"../resources/genome/betula/sample_sets/{w.sample_set}.args",
            "--remove-unused-alternates": True
        }
    conda:
        "envs/gatk.yaml"
    script:
        "scripts/run_gatk.py"

# get the interval lists of the chunks to be created
rule split_intervals:
    input:
        ref="../resources/genome/betula/genome.fasta",
        index="../resources/genome/betula/genome.fasta.fai",
        vcf="results/vcf/betula/{sample_set}.vcf.gz",
        tbi="results/vcf/betula/{sample_set}.vcf.gz.tbi"
    output:
        [f"results/vcf/betula/{{sample_set}}/interval_lists/{str(i).zfill(4)}-scattered.interval_list" for i
         in range(config['n_chunks_vcf'])]
    params:
        n=config['n_chunks_vcf'],
        out_prefix='results/vcf/betula/{sample_set}/interval_lists'
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk SplitIntervals -R {input.ref} -L {input.vcf} --scatter-count {params.n} -O {params.out_prefix}"

# split the VCF file into chunks
rule split_vcf:
    input:
        vcf="results/vcf/betula/{sample_set}.vcf.gz",
        tbi="results/vcf/betula/{sample_set}.vcf.gz.tbi",
        ref="../resources/genome/betula/genome.fasta",
        index="../resources/genome/betula/genome.fasta.fai",
        dict="../resources/genome/betula/genome.dict",
        list=lambda w: f"results/vcf/betula/{{sample_set}}/interval_lists/{str(w.i).zfill(4)}-scattered.interval_list"
    output:
        "results/vcf/betula/{sample_set}/vcf/{i}.vcf.gz"
    conda:
        "envs/gatk.yaml"
    shell:
        "gatk SelectVariants -R {input.ref} -V {input.vcf} -L {input.list} -O {output}"

# parse SFS from VCF
rule parse_vcf:
    input:
        vcf="results/vcf/betula/{sample_set}/vcf/{i}.vcf.gz",
        ref="../resources/genome/betula/genome.fasta"
    output:
        "results/spectra/betula/{sample_set}/{stratifications}/{n}/{i,\d+}.csv"
    params:
        n=lambda w: int(w.n),
        stratifications=lambda w: w.stratifications.split('.')
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf.py"

# parse SFS from VCF
rule merge_spectra:
    input:
        lambda w: expand("results/spectra/betula/{sample_set}/{stratifications}/{n}/{i}.csv",i=range(
            config['n_chunks_vcf']),allow_missing=True)
    output:
        "results/spectra/betula/{sample_set}/{stratifications}/{n}/all.csv"
    conda:
        "envs/base.yaml"
    script:
        "scripts/merge_spectra.py"

# run joint inference from template
rule run_joint_inference_from_templates:
    input:
        "../resources/configs/shared/{config}/config.yaml"
    output:
        summary="results/fastdfe/templates/shared/{config}/summary.json",
        serialized="results/fastdfe/templates/shared/{config}/serialized.json"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_shared_dfe.py"

# load EST-SFS module
module est_sfs:
    snakefile:
        "rules/est-sfs.smk"
    config:
        {
            'vcf_in': "../resources/genome/betula/all.with_outgroups.subset.10000.vcf.gz",
            'vcf_out': "../resources/genome/betula/all.with_outgroups.subset.10000.polarized.vcf.gz",
            'wildcards': {},
            'ingroups': "../resources/genome/betula/sample_sets/birch.args",
            'outgroups': "../resources/genome/betula/sample_sets/outgroups.args",
            'basepath': "results/est-sfs/",
            'basepath_vcf': "results/est-sfs/",
            'max_sites': 1000000,
            'n_samples': 50,
            'n_outgroup': 3,
            'model': 1,
            'nrandom': 10,
            'log_level': 20
        }

# load all rules from EST-SFS module
use rule * from est_sfs as *

# annotate synonymy
rule annotate_synonymy:
    input:
        vcf="../resources/genome/{species}/{name}.vcf.gz",
        ref="../resources/genome/{species}/genome.fasta",
        gff="../resources/genome/{species}/genome.gff.gz"
    output:
        "results/vcf/{species}/{name}.synonymy.vcf.gz"
    conda:
        "envs/base.yaml"
    script:
        "scripts/annotate_synonymy.py"

# annotate degeneracy
rule annotate_degeneracy:
    input:
        vcf="../resources/genome/{species}/{name}.vcf.gz",
        ref="../resources/genome/{species}/genome.fasta",
        gff="../resources/genome/{species}/genome.gff.gz"
    output:
        "results/vcf/{species}/{name}.degeneracy.vcf.gz"
    conda:
        "envs/base.yaml"
    script:
        "scripts/annotate_degeneracy.py"

# predict the variants' effects with VEP
rule annotate_vep_betula:
    input:
        vcf="../resources/genome/betula/{name}.vcf.gz",
        ref="../resources/genome/betula/genome.fasta",
        gff="../resources/genome/betula/genome.gff.gz",
        tbi="../resources/genome/betula/genome.gff.gz.tbi"
    output:
        "results/vcf/betula/{name}.vep.vcf.gz",
        "results/vcf/betula/{name}.vep.vcf.gz_summary.html"
    params:
        species="betula"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# download human chromosome-level VCF
rule download_vcf_chr_sapiens:
    output:
        "../resources/genome/sapiens/{chr}.vcf.gz"
    params:
        url="https://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/1000_genomes_project/release/"
            "20181203_biallelic_SNV/ALL.{chr}.shapeit2_integrated_v1a.GRCh38.20181129.phased.vcf.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download human chromosome-level FASTA
rule download_fasta_chr_sapiens:
    output:
        "../resources/genome/sapiens/{chr}.fasta.gz"
    params:
        url="https://hgdownload.soe.ucsc.edu/goldenPath/hg38/chromosomes/{chr}.fa.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download human chromosome-level GFF
rule download_gff_chr_sapiens:
    output:
        "../resources/genome/sapiens/chr{name,\d+}.gff3.gz"
    params:
        url="http://ftp.ensembl.org/pub/release-109/gff3/homo_sapiens/Homo_sapiens.GRCh38.109.chromosome.{name}.gff3.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# sort GFF
rule sort_and_compress_gff:
    input:
        "{path}.{ext}.gz"
    output:
        "{path}.sorted.{ext,gtf|gff|gff3}"
    conda:
        "envs/tabix.yaml"
    shell:
        """
        zgrep '^#' {input} > {output}
        zgrep -v '^#' {input} | sort -V -k1,1 -k4,4n >> {output}
        """

# Sort and recompress GFF
rule sort_and_recompress_gff:
    input:
        "{path}.{ext}.gz"
    output:
        "{path}.corrected.{ext,gtf|gff|gff3}.gz"
    conda:
        "envs/tabix.yaml"
    shell:
        """
        (zcat < {input} | grep '^#' ; zcat < {input} | grep -v '^#' | sort -V -k1,1 -k4,4n) | bgzip -c > {output}
        """

# decompress fasta file
rule decompress_bgzip_fasta:
    input:
        "{path}.fasta.gz"
    output:
        "{path,.*}.fasta"
    conda:
        "envs/tabix.yaml"
    shell:
        "bgzip {input} -cd > {output}"

# recompress FASTA file
rule recompress_bgzip_fasta:
    input:
        "{path}.fasta.gz"
    output:
        "{path}.recompressed.fasta.gz"
    conda:
        "envs/tabix.yaml"
    shell:
        "gzip -cd {input} | bgzip -c > {output}"


# predict the variants' effects with VEP
rule annotate_vep_sapiens:
    input:
        vcf="../resources/genome/sapiens/{name}.vcf.gz",
        ref="../resources/genome/sapiens/{name}.fasta",
        gff="../resources/genome/sapiens/{name}.sorted.gff3.gz",
        tbi="../resources/genome/sapiens/{name}.sorted.gff3.gz.tbi"
    output:
        "results/vcf/sapiens/{name,[a-z0-9]+}.vep.vcf.gz",
        "results/vcf/sapiens/{name}.vep.vcf.gz_summary.html"
    params:
        species="homo_sapiens"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# predict the variants' effects with VEP
rule annotate_vep_sapiens_test:
    input:
        vcf="../resources/genome/sapiens/{name}_test.vcf.gz",
        ref="../resources/genome/sapiens/{name}.fasta",
        gff="../resources/genome/sapiens/{name}.sorted.gff3.gz",
        tbi="../resources/genome/sapiens/{name}.sorted.gff3.gz.tbi"
    output:
        "results/vcf/sapiens/{name}_test.vep.vcf.gz",
        "results/vcf/sapiens/{name}_test.vep.vcf.gz_summary.html"
    params:
        species="homo_sapiens"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# predict the variants' effects with SnpEff
rule annotate_snpeff_sapiens:
    input:
        vcf="../resources/genome/sapiens/{name}.vcf.gz"
    output:
        "results/vcf/sapiens/{name}.snpeff.vcf.gz"
    params:
        species="hg38"  # The name of the SnpEff database for the species
    conda:
        "envs/snpeff.yaml"
    script:
        "scripts/annotate_snpeff.py"

# generate a requirements.txt using poetry
rule generate_requirements_poetry:
    output:
        base="envs/requirements.txt",
        base_snakemake=".snakemake/conda/requirements.txt",
        testing="envs/requirements_testing.txt",
        testing_snakemake=".snakemake/conda/requirements_testing.txt",
        docs="../docs/requirements.txt"
    conda:
        "envs/build.yaml"
    shell:
        """
            poetry update
            poetry export -f requirements.txt --without-hashes -o {output.base}
            poetry export -f requirements.txt --without-hashes -o {output.base_snakemake}
            poetry export --with dev -f requirements.txt --without-hashes -o {output.testing}
            poetry export --with dev -f requirements.txt --without-hashes -o {output.testing_snakemake}
            poetry export --with dev -f requirements.txt --without-hashes -o {output.docs}
        """

# download human chromosome-level VCF
rule download_vcf_hgdp:
    output:
        "results/vcf/hgdp/{chr}/opts.vcf.gz"
    params:
        url="https://ngs.sanger.ac.uk//production/hgdp/hgdp_wgs.20190516/hgdp_wgs.20190516.full.chr{chr}.vcf.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# create test sets ofr HGDP VCFs
rule subset_vcf_hgdp:
    input:
        "results/vcf/hgdp/{chr}/{opts}.vcf.gz"
    output:
        "results/vcf/hgdp/{chr}/{opts}.subset.{n}.vcf.gz"
    params:
        n=lambda w: int(w.n)
    conda:
        "envs/tabix.yaml"
    shell:
        """
        (zcat < {input} | grep "^#" ; zcat < {input} | grep -v "^#" | head -n {params.n} || true) | bgzip -c > {output}
        """

# download human chromosome-level FASTA
rule download_fasta_hgdp:
    output:
        "results/fasta/hgdp/{chr}.fasta.gz"
    params:
        url="http://ftp.ensembl.org/pub/release-109/fasta/homo_sapiens/dna/Homo_sapiens.GRCh38.dna.chromosome.{chr}.fa.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download human chromosome-level GFF
rule download_gff_hgdp:
    output:
        "results/gff/hgdp/{chr}.gff3.gz"
    params:
        url="http://ftp.ensembl.org/pub/release-109/gff3/homo_sapiens/Homo_sapiens.GRCh38.109.chromosome.{chr}.gff3.gz"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download population metadata for HGDP
rule download_hgdp_population_metadata:
    output:
        "results/sample_sets/hgdp/all.csv"
    params:
        url="https://ngs.sanger.ac.uk//production/hgdp/hgdp_wgs.20190516/metadata/hgdp_wgs.20190516.metadata.txt"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# predict the variants' effects with VEP
rule annotate_vep_hgdp:
    input:
        vcf="results/vcf/hgdp/{chr}/{opts}.vcf.gz",
        ref="results/fasta/hgdp/{chr}.recompressed.fasta.gz",
        gff="results/gff/hgdp/{chr}.corrected.gff3.gz",
        tbi="results/gff/hgdp/{chr}.corrected.gff3.gz.tbi"
    output:
        "results/vcf/hgdp/{chr}/{opts}.vep.vcf.gz",
        "results/vcf/hgdp/{chr}/{opts}.vep.vcf.gz_summary.html"
    params:
        species="homo_sapiens"
    conda:
        "envs/vep.yaml"
    script:
        "scripts/annotate_vep.py"

# convert VCF file to PLINK format
rule convert_plink_sample_set_hgdp:
    input:
        "results/vcf/hgdp/{chr}/{opts}.vcf.gz"
    output:
        multiext("results/plink/hgdp/{chr}/{opts}.",'bed','bim','fam','log')
    conda:
        "envs/plink.yaml"
    script:
        "scripts/convert_plink.py"

# subset the given sample set by the 'val' in 'col'
# of the given CSV file
rule filter_sample_set_hgdp:
    input:
        "results/sample_sets/hgdp/all.csv"
    output:
        "results/sample_sets/hgdp/{population}.csv"
    conda:
        "envs/base.yaml"
    params:
        filter_col='population',
        filter_val='{population}'
    script:
        "scripts/filter_sample_set.py"

# create a list of sample names from the samples file
rule derive_sample_list_hgdp:
    input:
        "results/sample_sets/hgdp/{population}.csv"
    output:
        "results/sample_lists/hgdp/{population}.args"
    params:
        name_col='sample'
    conda:
        "envs/base.yaml"
    script:
        "scripts/derive_sample_list.py"

# generate PCA plot
rule plot_pca_hgdp:
    input:
        bed="results/plink/hgdp/{chr}/{opts}.bed",
        samples="results/sample_sets/hgdp/{population}.csv"
    output:
        "results/graphs/pca/hgdp/{chr}/{opts}/{population}.png"
    params:
        marker_size=15,
        name_col='sample',
        label_col='population',
        cbar=False,
        legend=True,
        subsample=10000
    conda:
        "envs/pca.yaml"
    script:
        "scripts/plot_pca.py"

# parse spectra from human chromosome-level VCF
rule parse_vcf_hgdp:
    input:
        vcf="results/vcf/hgdp/{chr}/{opts}.vcf.gz",
        fasta="results/fasta/hgdp/{chr}.fasta.gz",
        gff="results/gff/hgdp/{chr}.corrected.gff3.gz",
        samples="results/sample_lists/hgdp/{population}.args"
    output:
        csv="results/spectra/hgdp/{chr,\d+}/{opts}.n.{n}/{population}.csv",
        png="results/graphs/spectra/hgdp/{chr,\d+}/{opts}.n.{n}/{population}.png"
    params:
        aliases=lambda w: {f"chr{w.chr}": [f"{w.chr}"]},
        n=lambda w: int(w.n),
        chr="{chr}"
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf_hgdp.py"

# merge spectra for hgdp
rule merge_spectra_hgdp:
    input:
        expand("results/spectra/hgdp/{chr}/{opts}.n.{n}/{population}.csv",
            chr=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10],allow_missing=True)
    output:
        "results/spectra/hgdp/all/{opts}.n.{n}/{population}.csv"
    conda:
        "envs/base.yaml"
    script:
        "scripts/merge_spectra.py"

# plot SFS for hgdp
rule plot_spectra_hgdp:
    input:
        "results/spectra/hgdp/all/{opts}.n.{n}/{population}.csv"
    output:
        "results/graphs/spectra/hgdp/all/{opts}.n.{n}/{population}.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/plot_spectra.py"

# prepare config file for HGDP
rule prepare_config_hgdp:
    input:
        config="../resources/configs/HGDP/{tool}/{config}.yaml",
        spectra="results/spectra/hgdp/{chr}/{opts}.n.{n}/{population}.csv"
    output:
        "results/{tool}/hgdp/config/{chr}/{opts}.n.{n}.{config}/{population}.yaml"
    conda:
        "envs/base.yaml"
    script:
        "scripts/prepare_config_hgdp.py"

# infer DFE from spectra for the HGDP data set using fastDFE
rule infer_dfe_hgdp_fastdfe:
    input:
        "results/fastdfe/hgdp/config/{chr}/{opts}.n.{n}.{config}/{population}.yaml"
    output:
        serialized="results/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        summary="results/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.summary.json",
        dfe="results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.dfe.png",
        spectra="results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.spectra.png",
        params="results/graphs/fastdfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe.py"

# infer DFE from spectra for the HGDP data set using polyDFE
rule infer_dfe_hgdp_polydfe:
    input:
        "results/polydfe/hgdp/config/{chr}/{opts}.n.{n}.{config}/{population}.yaml"
    output:
        serialized="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        summary="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.summary.json",
        polydfe="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.polydfe.txt"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# create bootstrap samples for polyDFE
rule create_bootstrap_hgdp_polydfe:
    input:
        "results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        config['polydfe_postprocessing_source']
    output:
        "results/polydfe/hgdp/config/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.yaml"
    conda:
        dev_env
    script:
        "scripts/create_bootstrap_polydfe.py"

# infer DFE from bootstrapped SFS using polyDFE
rule infer_dfe_hgdp_polyDFE_bootstrap:
    input:
        "results/polydfe/hgdp/config/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.yaml",
        config['polydfe_postprocessing_source']
    output:
        polydfe="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.polydfe.txt",
        summary="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.summary.json",
        serialized="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.serialized.json"
    params:
        bin=lambda w: '../resources/polydfe/bin/polyDFE-2.0-macOS-64-bit' if is_macos else 'polydfe'
    conda:
        dev_env
    script:
        "scripts/infer_dfe_polydfe.py"

# merge bootstrap samples for polyDFE
rule merge_bootstraps_hgdp_polydfe:
    input:
        config['polydfe_postprocessing_source'],
        original="results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.serialized.json",
        bootstraps=lambda w: expand("results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{i}/{population}.serialized.json",i=range(
            config['n_bootstraps_hgdp']),allow_missing=True)
    output:
        "results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{population}.serialized.json"
    conda:
        dev_env
    script:
        "scripts/merge_bootstraps_polydfe.py"

# plot polyDFE inference results
rule plot_polydfe_inference_hgdp:
    input:
        "results/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/bootstraps/{population}.serialized.json"
    output:
        dfe_discretized="results/graphs/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.dfe.png",
        mle_params="results/graphs/polydfe/hgdp/inference/{chr}/{opts}.n.{n}.config.{config}/{population}.params.png",
    conda:
        dev_env
    script:
        "scripts/plot_polydfe_inference.py"

# download drosophila data from https://doi.org/10.1371/journal.pbio.3001775
rule download_sfs_covariate_tables_drosophila:
    output:
        "results/sfs_covariates/drosophila.csv"
    params:
        url="https://gitlab.gwdg.de/molsysevol/supplementarydata_geneage/-/raw/master/Data/S31_Data.csv"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# download arabidopsis data from https://doi.org/10.1371/journal.pbio.3001775
rule download_sfs_covariate_tables_arabidopsis:
    output:
        "results/sfs_covariates/arabidopsis.csv"
    params:
        url="https://gitlab.gwdg.de/molsysevol/supplementarydata_geneage/-/raw/master/Data/S32_Data.csv"
    conda:
        "envs/wget.yaml"
    shell:
        "wget -O {output} {params.url}"

# prepare config file for hominidae
rule prepare_config_hominidae:
    input:
        neutral="../resources/SFS/hominidae/uSFS/{type}_4fold_all_sfs.txt",
        selected="../resources/SFS/hominidae/uSFS/{type}_0fold_all_sfs.txt"
    output:
        "results/fastdfe/hominidae/{type}.yaml"
    conda:
        "envs/base.yaml"
    script:
        "scripts/prepare_config_hominidae.py"

# infer DFE from spectra for the hominidae data set using fastDFE
rule infer_dfe_hominidae_fastdfe:
    input:
        "results/fastdfe/hominidae/{type}.yaml"
    output:
        serialized="results/fastdfe/hominidae/{type,[a-zA-Z]}.serialized.json",
        summary="results/fastdfe/hominidae/{type,[a-zA-Z]}.summary.json",
        dfe="results/graphs/fastdfe/hominidae/{type,[a-zA-Z]}.dfe.png",
        spectra="results/graphs/fastdfe/hominidae/{type,[a-zA-Z]}.spectra.png",
        params="results/graphs/fastdfe/hominidae/{type,[a-zA-Z]}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/infer_dfe.py"

# create config files for checking covariates
rule create_config_covariates_hominidae:
    input:
        neutral=expand('../resources/SFS/hominidae/uSFS/{t}_4fold_all_sfs.txt',t=hominidae),
        selected=expand('../resources/SFS/hominidae/uSFS/{t}_0fold_all_sfs.txt',t=hominidae)
    output:
        "results/fastdfe/hominidae/cov/{param}.yaml"
    params:
        param="{param}",
        types=hominidae
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config_covariates_hominidae.py"

# run joint inference using covariates
rule infer_dfe_hominidae_covariates:
    input:
        "results/fastdfe/hominidae/cov/{param}.yaml"
    output:
        serialized="results/fastdfe/hominidae/cov/{param}.serialized.json",
        summary="results/fastdfe/hominidae/cov/{param}.summary.json",
        dfe="results/graphs/fastdfe/hominidae/cov/{param}.dfe.png",
        spectra="results/graphs/fastdfe/hominidae/cov/{param}.spectra.png",
        params="results/graphs/fastdfe/hominidae/cov/{param}.params.png"
    conda:
        "envs/base.yaml"
    script:
        "scripts/check_covariates.py"

# parse betula SFS directly from original VCF file
rule parse_vcf_betula:
    input:
        vcf='../resources/genome/betula/all.vcf.gz',
        ref='../resources/genome/betula/genome.fasta',
        gff='../resources/genome/betula/genome.gff.gz',
        samples='../resources/genome/betula/sample_sets/{sample_set}.args'
    output:
        csv="results/sfs/betula/{sample_set}/{n}/sfs.csv",
        png="results/graphs/sfs/betula/{sample_set}/{n}/sfs.png"
    params:
        n=lambda w: int(w.n)
    conda:
        "envs/base.yaml"
    script:
        "scripts/parse_vcf_betula.py"

# create config files for checking covariates
rule create_config_covariates_arabidopsis:
    input:
        "results/sfs_covariates/arabidopsis.csv"
    output:
        "results/fastdfe/arabidopsis/cov/{param}.{col}.yaml"
    params:
        param="{param}",
        col="{col}"
    conda:
        "envs/base.yaml"
    script:
        "scripts/create_config_covariates_arabidopsis.py"

# run joint inference using covariates
rule infer_dfe_arabidopsis_covariates:
    input:
        "results/fastdfe/arabidopsis/cov/{param}.{col}.yaml"
    output:
        serialized="results/fastdfe/arabidopsis/cov/{param}.{col}.serialized.json",
        summary="results/fastdfe/arabidopsis/cov/{param}.{col}.summary.json",
        dfe="results/graphs/fastdfe/arabidopsis/cov/{param}.{col}.dfe.png",
        spectra="results/graphs/fastdfe/arabidopsis/cov/{param}.{col}.spectra.png",
        params="results/graphs/fastdfe/arabidopsis/cov/{param}.{col}.params.png"
    params:
        legend=dict(bbox_to_anchor=(1, 0.5),ncol=3,fontsize=8)
    conda:
        "envs/base.yaml"
    script:
        "scripts/check_covariates.py"
